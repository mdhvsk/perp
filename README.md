# RAG-Powered Health/Nutrition Answer Engine

A full-stack application that provides AI-powered answers to research queries using RAG (Retrieval Augmented Generation) technology. The system retrieves relevant research papers from arXiv, generates accurate responses using OpenAI's LLM, and maintains context through vector similarity search.

## ğŸš€ Features

- **Real-time Research Paper Analysis**: Fetches and analyzes papers from arXiv API in real-time
- **Intelligent Answer Generation**: Uses OpenAI's GPT models for generating accurate, context-aware responses
- **Vector Similarity Search**: Implements efficient document retrieval using Pinecone
- **Persistent Chat History**: Stores all conversations and context in Supabase
- **Modern Web Interface**: Built with Next.js (App Router) for a responsive user experience

## ğŸ› ï¸ Technology Stack

### Frontend
- Next.js
- React
- TailwindCSS
- TypeScript

### Backend
- FastAPI
- Python 3.9+
- OpenAI API
- Pinecone
- Supabase

## ğŸ“‹ Prerequisites

- Python 3.9+
- Node.js 16+
- OpenAI API key
- Pinecone API key
- Supabase account and credentials
- arXiv API access


## Architecture Diagram 
<img width="911" alt="Screenshot 2025-01-05 at 8 43 45â€¯PM" src="https://github.com/user-attachments/assets/2ad44848-9b53-4d32-baf0-90bb649764da" />

## âš™ï¸ Environment Variables

```bash
# Backend (.env)
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

# Frontend (.env.local)
NEXT_PUBLIC_API_URL=your_backend_url
```

## ğŸš€ Installation & Setup

1. Clone the repository:
```bash
git clone https://github.com/mdhvsk/perp.git
cd perp
```

2. Set up the backend:
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. Set up the frontend:
```bash
cd frontend
npm install
```

4. Start the development servers:

Backend:
```bash
uvicorn backend.main:app --reload
```

Frontend:
```bash
npm run dev
```

## ğŸ¯ Usage

1. Visit `http://localhost:3000` in your browser
2. Enter your research query in the chat interface
3. The system will:
   - Retrieve relevant papers from arXiv
   - Generate embeddings using OpenAI
   - Store and search vectors in Pinecone
   - Generate a comprehensive answer using OpenAI's LLM
   - Store the conversation in Supabase

## ğŸ”„ RAG Pipeline

1. **Document Retrieval**: Fetches relevant research papers from arXiv based on user query
2. **Embedding Generation**: Creates embeddings for documents using OpenAI's embedding model
3. **Vector Storage**: Stores embeddings in Pinecone for efficient similarity search
4. **Context Retrieval**: Retrieves relevant context based on query similarity
5. **Answer Generation**: Generates comprehensive answers using OpenAI's LLM

## ğŸ“š Dependencies & APIs

- **arXiv API**: Primary source for research papers
- **OpenAI API**: Used for:
  - Text embeddings (text-embedding-ada-002)
  - LLM generation (gpt-4-turbo)
- **Pinecone**: Vector database for similarity search
- **Supabase**: PostgreSQL database for chat storage

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.


